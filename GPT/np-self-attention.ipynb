{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a12125b1c14d700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T10:17:19.008896Z",
     "start_time": "2023-12-21T10:17:18.521555Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b7f27aca4081ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T10:17:19.839532Z",
     "start_time": "2023-12-21T10:17:19.827372Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define softmax operation that works independently on each column\n",
    "def softmax_cols(data_in):\n",
    "    # Exponentiate all of the values\n",
    "    exp_values = np.exp(data_in)\n",
    "    # Sum over columns\n",
    "    denom = np.sum(exp_values, axis=0)\n",
    "    # Replicate denominator to N rows\n",
    "    denom = np.matmul(np.ones((data_in.shape[0], 1)), denom[np.newaxis, :])\n",
    "    # Compute softmax\n",
    "    softmax = exp_values / denom\n",
    "    # return the answer\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T10:17:20.555720Z",
     "start_time": "2023-12-21T10:17:20.551906Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Now let's compute self attention in matrix form\n",
    "def self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
    "    # 1. Compute queries, keys, and values\n",
    "    queries = np.matmul(omega_q, X) + beta_q\n",
    "    keys = np.matmul(omega_k, X) + beta_k\n",
    "    values = np.matmul(omega_v, X) + beta_v\n",
    "\n",
    "    # 2. Compute dot products\n",
    "    transposed_keys = np.transpose(keys)\n",
    "    dot_product = np.matmul(transposed_keys, queries)\n",
    "\n",
    "    # 3. Apply softmax to calculate attentions\n",
    "    attention = softmax_cols(dot_product)\n",
    "\n",
    "    # 4. Weight values by attentions\n",
    "    X_prime = np.matmul(values, attention)\n",
    "\n",
    "    return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9c7bbeaf1aa01a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T10:17:20.974611Z",
     "start_time": "2023-12-21T10:17:20.971869Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(3)\n",
    "# Number of inputs\n",
    "N = 3\n",
    "# Number of dimensions of each input\n",
    "D = 4\n",
    "# Create an empty list\n",
    "all_x = []\n",
    "# Create elements x_n and append to list\n",
    "for n in range(N):\n",
    "    all_x.append(np.random.normal(size=(D, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6987b5f2e501ce8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T10:17:21.494243Z",
     "start_time": "2023-12-21T10:17:21.488480Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(0)\n",
    "\n",
    "# Choose random values for the parameters\n",
    "omega_q = np.random.normal(size=(D, D))\n",
    "omega_k = np.random.normal(size=(D, D))\n",
    "omega_v = np.random.normal(size=(D, D))\n",
    "beta_q = np.random.normal(size=(D, 1))\n",
    "beta_k = np.random.normal(size=(D, 1))\n",
    "beta_v = np.random.normal(size=(D, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "951aebdac2e8cd6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T10:17:21.923404Z",
     "start_time": "2023-12-21T10:17:21.919395Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.94744244  1.64201168  1.61949281]\n",
      " [-0.24348429 -0.08470004 -0.06641533]\n",
      " [-0.91310441  4.02764044  3.96863308]\n",
      " [-0.44522983  2.18690791  2.15858316]]\n"
     ]
    }
   ],
   "source": [
    "# Copy data into matrix\n",
    "X = np.zeros((D, N))\n",
    "X[:, 0] = np.squeeze(all_x[0])\n",
    "X[:, 1] = np.squeeze(all_x[1])\n",
    "X[:, 2] = np.squeeze(all_x[2])\n",
    "\n",
    "# Run the self attention mechanism\n",
    "X_prime = self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
    "\n",
    "# Print out the results\n",
    "print(X_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b72b4dcd03c034",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The values are quite extreme (one is very close to one and the others are very close to zero. Now we'll fix this problem by using scaled dot-product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea3a0430b11d2f2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T10:23:41.022807Z",
     "start_time": "2023-12-21T10:23:41.019428Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's compute self attention in matrix form\n",
    "def scaled_dot_product_self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
    "    # 1. Compute queries, keys, and values\n",
    "    queries = np.matmul(omega_q, X) + beta_q\n",
    "    keys = np.matmul(omega_k, X) + beta_k\n",
    "    values = np.matmul(omega_v, X) + beta_v\n",
    "\n",
    "    # 2. Compute dot products\n",
    "    transposed_keys = np.transpose(keys)\n",
    "    dot_product = np.matmul(transposed_keys, queries)\n",
    "\n",
    "    # 3. Scale the dot products as in equation 12.9\n",
    "    scaled_dot_product = dot_product / np.sqrt(keys.shape[0])\n",
    "\n",
    "    # 4. Apply softmax to calculate attentions\n",
    "    attention = softmax_cols(scaled_dot_product)\n",
    "\n",
    "    # 5. Weight values by attentions\n",
    "    X_prime = np.matmul(values, attention)\n",
    "\n",
    "    return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44c6158d8ce74b0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T10:23:41.785077Z",
     "start_time": "2023-12-21T10:23:41.781795Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.97411966  1.59622051  1.32638014]\n",
      " [-0.23738409 -0.09516106  0.13062402]\n",
      " [-0.72333202  3.70194096  3.02371664]\n",
      " [-0.34413007  2.01339538  1.6902419 ]]\n"
     ]
    }
   ],
   "source": [
    "# Run the self attention mechanism\n",
    "X_prime = scaled_dot_product_self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
    "\n",
    "# Print out the results\n",
    "print(X_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f2eb885c99ad92",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
