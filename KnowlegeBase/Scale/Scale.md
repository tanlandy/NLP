# Scale

研究的主要焦点在于通过持续预训练（continual pretraining）来增强模型处理长上下文的能力

数据工程方法：

这包括在1-5B个标记的每个源长度上采样数据上持续预训练全注意力模型。

500M到5B个标记的数据量足以使模型在128K上下文中的任何位置检索信息

“按源长度上采样”（per-source length upsampling）的方法，即在每个领域内部上采样长序列，同时保持原始数据混合的领域比例不变。

通过持续预训练也能有效提升模型处理长上下文的能力。这验证了所提出方法的有效性，并表明在某些情况下，数据工程比模型架构的修改更为关键。
