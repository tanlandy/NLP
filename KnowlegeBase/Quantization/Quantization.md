# Quantization

## 参数的数据格式

![data_type](tf32-Mantissa-chart-hi-res-FINAL.png)

记忆方式：58同城

5是fp16的指数位大小
8是fp32，bf16的指数位大小

5位指数位可以表示的值范围是2^5，即0到31
但实际上用于表示指数的是这个值减去偏移量15
因此实际的指数范围是-14到+15

5位指数位对应的范围是
$2^{2^{5-1}} = 2^{2^4} = 2^{16}$

8位指数位对应的范围是
$2^{2^{8-1}} = 2^{2^7} = 2^{128}$

目前，Ada Lovelace及其之后的架构，支持bf16格式。

## int8量化

来源：Meta团队于22年11月提出

![LLM.int8()步骤](<截屏2024-03-28 下午3.03.56.png>)

### 基础知识

#### 最大绝对值量化

将参数scale到8-bit range[-127, 127]。具体方式是把每个数乘以127/最大值

#### 零点量化

reducing the quantization error for asymmetric distributions.

它不仅使用数据的动态范围来缩放数据，还通过一个称为零点（zeropoint）的参数来调整数据，使得量化后的数据能够利用整个表示范围。零点是通过对数据进行平移操作来确定的，目的是使得量化后的数据的均值移动到量化类型的中点值。

具体来说，零点量化的步骤如下：

1. 计算动态范围：首先计算输入数据的动态范围，即最大值和最小值之差。
2. 确定零点：然后确定零点值，这通常是通过将动态范围的一半加到最小值上得到的，或者通过其他方法确定，以确保量化后的数据能够覆盖整个表示范围。
3. 缩放数据：接着，将输入数据乘以一个缩放因子，该因子基于动态范围和量化类型的位宽来确定。
4. 应用零点：最后，将缩放后的数据加上零点值，以确保数据的分布均匀地映射到量化类型的整个范围内。

### LLM.int8()

核心是结合了两种量化技术：向量级量化（vector-wise quantization）和混合精度分解（mixed-precision decomposition）

#### 向量级量化

传统的量化方法通常使用单一的缩放常数来量化整个张量。向量级量化则为矩阵乘法中的每个内积操作分配不同的归一化常数。

这种方法通过分别量化矩阵的行和列来提高量化精度，从而在保持性能的同时减少了内存需求。

#### 混合精度分解（核心）

为了处理在大规模Transformer模型中出现的极端特征值（outliers）而引入

这种方法将极端特征值从其他特征中分离出来，并为这些维度使用16位精度进行矩阵乘法，而其他99.9%的维度则使用8位精度。

为了有效地量化这些极端特征值，研究者们将它们从正常的特征维度中分离出来。这是通过创建一个包含所有具有极端特征值的特征维度的集合来实现的。

在完成所有特征维度的量化矩阵乘法后，将极端特征维度的结果与其他特征维度的结果合并。

## int4

来源：微软deepspeed团队于23年5月提出的

效果：
对于编码器模型（如BERT），W4A4量化在分类任务上没有造成准确性损失。
对于编码器-解码器模型（如BART），在摘要任务上，W4A4量化的准确性差异可以忽略不计。
然而，对于解码器模型（如GPT），在自回归生成任务上，W4A4量化导致了相对较大准确性下降。

### 权重量化

权重矩阵被分成g组，每组进行独立的量化
对于每一组权重，使用对称或非对称量化策略进行量化

### 激活量化

激活是动态的，因此需要在推理时动态量化。论文中提到了两种激活量化方法：静态量化和动态量化。
静态量化使用训练数据来校准比例因子S并在推理时固定S，这种方法可以减少延迟但限制了激活的量化表示范围。
动态量化则采用更细粒度的token-wise动态量化，使用每个token的最小/最大范围进行量化。

## AWQ

来源：MIT于23年10月提出的

核心思想是在量化过程中保护那些对模型性能影响较大的权重，即显著的权重。并不是将所有参数进行量化，而是保留很小比例对于模型效果至关重要的参数的精度，量化其他参数。最后效果是4bit量化后并不会有明显的性能损失。

AWQ的目标是找到一个缩放因子α，使得量化后的权重（通过乘以α）与原始权重的输出差异最小

搜索空间基于激活的幅度，即权重通道的重要性是由激活规模决定的。AWQ使用一个单一的超参数α来平衡保护显著和非显著通道的需求

### AWQ具体步骤

1. 确定显著权重：首先，AWQ识别出那些在激活分布中对应较大激活幅度的权重通道，因为这些通道处理的特征更加重要。这一步不直接依赖于权重的大小，而是通过激活的分布来间接识别。
2. 分析量化误差：AWQ分析了仅量化权重时的误差来源。对于一个权重组或块w，线性操作可以表示为y = wx，量化后的对应操作为y = Q(w)x，其中Q(w)是一个量化函数。
3. 通过缩放减少量化误差：AWQ提出了一种通过每通道缩放的方式来减少显著权重的量化误差。具体来说，对于一个给定的权重元素w，如果将其乘以一个大于1的缩放因子s，并对输入x进行相应的逆缩放，那么量化函数Q将映射到一个新的量化尺度上，从而减少相对量化误差。
4. 优化缩放因子：AWQ通过自动搜索最优的每输入通道缩放因子来最小化量化误差。这个优化目标是通过定义一个损失函数L(s)来实现的，其中L(s)衡量了量化权重和原始权重之间的输出差异。通过在[0, 1]区间内进行快速网格搜索，可以找到最佳的缩放因子α，以平衡保护显著和非显著通道。
5. 权重剪切：为了进一步减少量化误差，AWQ还应用了权重剪切技术。权重剪切有助于减少量化后的权重值范围，从而降低量化误差。

## GPTQ

来源：23年5月提出

GPTQ是一种新型的一次性权重量化方法
This merges the name of the OPT model family with the abbreviation for post-training quantization (PTQ).

### OBS框架

OBQ方法按层进行量化，对于网络中的每一层，它解决一个重建问题：找到一组量化权重，使得它们与全精度层输出之间的平方误差最小。

OBQ量化过程包括以下步骤：

1. 初始化：首先计算Hessian矩阵HF，它是权重的二阶导数矩阵，用于量化过程中的权重更新。
2. 权重量化：逐个量化权重，每次量化一个权重时，都会更新所有尚未量化的权重，以补偿量化单个权重所产生的误差。
3. 权重更新：使用Hessian矩阵和量化误差来更新剩余的权重，确保量化后的模型尽可能接近原始模型的输出。

因为OBQ的运行时间与权重矩阵的尺寸成三次方关系。这意味着对于非常大的模型，OBQ可能变得不切实际。

### GPTQ细节

GPTQ方法在OBQ的基础上进行了重大改进，特别是在处理大规模模型时的计算效率上。GPTQ通过引入任意顺序量化、懒惰批量更新和Cholesky重构等技术，显著提高了量化过程的扩展性和效率。

GPTQ量化的特性：

1. 任意顺序量化：GPTQ发现，在大型、参数量丰富的层上，按照任意顺序量化权重的效果与按照最优顺序量化的效果相差不大。这意味着可以采用简化的量化策略，而不会显著损失精度。
2. 懒惰批量更新：为了提高计算效率，GPTQ采用了懒惰批量更新策略。这种方法通过批量处理更新，而不是逐个权重更新，从而更好地利用GPU的计算能力。
3. Cholesky重构：为了解决大规模模型中可能出现的数值不稳定性问题，GPTQ采用了Cholesky分解来预先计算Hessian矩阵的逆，这是一种数值稳定的算法，可以有效地处理大规模矩阵运算。

GPTQ量化的过程：

GPTQ方法按层进行量化，对于模型中的每个线性变换层，执行以下步骤：

1. 计算Hessian矩阵：首先计算该层的Hessian矩阵，这是一个描述权重相对于损失函数的二阶导数信息的矩阵。这个矩阵对于后续的量化过程至关重要，因为它提供了权重之间相互作用的信息。
2. Cholesky分解：为了数值稳定性和计算效率，GPTQ使用Cholesky分解来表示Hessian矩阵的逆。这种分解允许在量化过程中高效地更新权重。
对于每一层的权重矩阵W，GPTQ执行以下量化和更新步骤：
3. 量化权重：选择一个权重量化的顺序，可以是任意顺序，因为论文中发现任意顺序与最优顺序之间的性能差异不大。对于每个选定的权重，将其量化到最接近的量化级别。
4. 计算量化误差：对于每个量化后的权重，计算其与原始权重之间的误差。
5. 更新权重：使用Hessian矩阵的逆和量化误差来更新剩余的未量化权重。这个过程是为了补偿量化一个权重可能引入的误差。

Hessian矩阵用于描述损失函数关于网络权重的二阶导数信息

## QLoRA

### 减少内存占用

1. NF4数据格式。相比4-bit int和4-bit float来说，理论上对于正态分布的权重是最优的
2. 双重量化：不仅模型的权重被量化，量化过程中使用的量化常数也被量化
3. 分页优化器：训练过程中，梯度的计算可能导致内存峰值，特别是在处理长序列时。QLoRA使用Nvidia的统一内存特性，可以在GPU内存不足时，自动讲数据页转移至CPU内存，从而进一步降低GPU内存占用

QLoRA用了两种数据类型：

- 低精度的存储数据类型NF4
- 计算时候使用的数据类型bf16

当参数量在使用的时候，会先从NF4反量化到bf16，然后进行矩阵运算

### NF4

确保量化后的数据类型有0点，作者估计两个范围的分位数：

- 负数 $2^{3}$

- 正数 $2^{3} + 1$

然后统一这两组分位数

### 双重量化

在标准的量化过程中，模型的权重被量化为较低比特宽度的表示形式，例如从32位浮点数（FP32）量化到4位或8位整数（Int8）。这些量化操作需要使用量化常数（也称为量化尺度或比例因子），它们决定了输入值到量化值的映射关系。在双重量化中，首次量化的量化常数本身也会被量化，这就是所谓的“双重量化”。

计算过程：在双重量化的过程中，首先将首次量化的量化常数（cFP32）作为输入进行第二次量化，得到量化后的量化常数（cFP8）和第二级别的量化常数（cFP32）。在实际计算中，首先对输入权重进行首次量化，然后使用二次量化的量化常数进行反量化，最后在16位（BFloat16）精度下进行矩阵乘法。

### 效果

Our results consistently show that 4-bit QL O RA with NF4 data type matches 16bit full finetuning and 16-bit LoRA finetuning performance on academic benchmarks with well-established evaluation setups.
