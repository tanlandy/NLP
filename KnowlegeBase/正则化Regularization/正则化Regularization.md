# 正则化Regularization

## Resources

[知乎详解](https://zhuanlan.zhihu.com/p/137073968)

## 基本概念

L1距离：即曼哈顿距离。只能横着走和竖着走

L2距离：即欧式距离。两点间的直线距离

![基本概念](<截屏2024-03-19 下午2.35.02.png>)

## 损失函数

L1, L2都可以做损失函数使用

### L1损失函数

也叫最小绝对值偏差（LAD），绝对值损失函数（LAE）
总的说来，它是把目标值与估计值的绝对差值的总和最小化。

L = sum(yi - f(xi))

### L2损失函数

L2范数损失函数，也被称为最小平方误差（LSE）。它是把目标值与估计值的差值的平方和最小化。一般回归问题会使用此损失，**离群点**对次损失影响较大。

L = sum((yi - f(xi))^2)

### L1 VS. L2

L1损失函数相比于L2损失函数的鲁棒性更好。

因为L2范数将误差平方化（如果误差大于1，则误差会放大很多），模型的误差会比L1范数大的多，因此模型会对这种类型的样本更加敏感，这就需要调整模型来最小化误差。但是很大可能这种类型的样本是一个异常值，模型就需要调整以适应这种异常值，那么就导致训练模型的方向偏离目标了。

## 正则化

### 功能

正则化用来防止过拟合，提高模型的泛化能力：通过降低模型的复杂性。
过拟合表示函数考虑到了每一个样本点，也就是在某些小区间里，函数值变化很强烈

### L1正则化（Lasso正则化）

L1正则化通过向损失函数添加模型参数的绝对值之和作为惩罚项来工作。L1正则常被用来进行**特征选择**，主要原因在于L1正则化会使得较多的参数为0，从而产生稀疏解，我们可以将0对应的特征遗弃，进而用来选择特征。一定程度上L1正则也可以防止模型过拟合。

使用场景

1. 当你想进行特征选择时，即自动去除不重要的特征。
2. 当模型特征非常多，但你认为只有少数几个特征是重要的。

L(W)是未加正则项的损失，$\lambda$是超参数，用来控制正则化项的大小。
损失函数L = L(W) + $\lambda$sum(abs(wi))

L1正则化会使参数变成0的原因，是L1的倒数在>0是1，<0是-1，在=0处是0。对于wi小于1的时候，L2的惩罚项会越来越小，而L1还是会很大。所以L1会使参数为0，L2很难

### L2正则化（Ridge正则化）

L2正则化通过向损失函数添加模型参数的平方和作为惩罚项来工作。倾向于分配非零权重，即使是小的权重，也让它们均匀分布。主要用来防止模型过拟合，直观上理解就是L2正则化是对于大数值的权重向量进行严厉惩罚。鼓励参数是较小值，如果$w$小于1，那么$w^2$会更小。L2正则化有助于处理那些参数之间高度相关（多重共线性）的情况，因为它可以强制让这些参数的权重分散开来，减少它们之间的依赖。

损失函数L = L(W) + $\lambda$sum((wi)^2)
使用场景

1. 当数据特征之间存在高度相关性时。
2. 当你不需要进行特征选择，即认为大多数特征都是有用的。
