acclerate 整合了大模型分布式训练所需的所有的trick
safetensors 解决模型序列化的安全性问题
gradio 提供快速部署demo的方案

你更熟悉的深度学习框架是什么？为什么选择它？
然后是关于大模型的整体架构
有哪些省内存的大语言模型？训练方法？在消费级显卡上训练大模型的方法有了解过吗
是否参与过大规模语言模型的预训练或SFT？
关于SFT和RLHF之间的关系，为什么不用大规模的监督数据训练来代替强化学习对BERT和BART？的了解，他们的区别是什么
预训练方面，有哪些操作能让最后的performance变好
LLMs存在模型幻觉问题，请问如何处理？
请解释一下注意力机制？是如何工作的，它在大模型中的应用有哪些？
你有使用过分布式训练？吗？在大规模模型上采用分布式训练有什么挑战？
最后是transformer2八股，经典为什么要除以根号d和为什么要用layer norm不用batch
normlization,