# Notes

## Learning objects

1. Describe how RLHF uses human feedback to improve the performance and alignment of large language models
   - Instruct fine-tuned LLM -> RLHF -> Human-aligned LLM
2. Explain how data gathered from human labelers is used to train a reward model for RLHF
3. Define chain-of-thought prompting and describe how it can be used to improve LLMs reasoning and planning abilities
4. Discuss the challenges that LLMs face with knowledge cut-offs, and explain how information retrieval and augmentation techniques can overcome these challenges

### KL-Divergence

KL-Divergence, or Kullback-Leibler Divergence, is a concept often encountered in the field of reinforcement learning, particularly when using the Proximal Policy Optimization (PPO) algorithm. It is a mathematical measure of the difference between two probability distributions, which helps us understand how one distribution differs from another. 
更多信息见[trl](https://huggingface.co/blog/trl-peft)

### Models behaving

HHH

- helpful
- honest
- harmless

## Quiz



## Reading lists