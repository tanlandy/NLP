# 阿里云百炼RAG在企业场景的应用

## RAG流程

![RAG流程](<Screenshot 2024-01-31 at 3.49.10 PM.png>)

## 目前RAG的痛点
1. 文档预处理：文档种类多，结构复杂，文档切分语义完整性不易保障
2. 精确度：知识片段召回效果要求高，对最终效果影响大
3. 可解释性：生成的结果需要可溯源、可信任

### 阿里百炼介绍：


![阿里百炼](<Screenshot 2024-02-02 at 6.28.39 PM.png>)


## 技术体系

![技术体系](<Screenshot 2024-02-02 at 6.29.13 PM.png>)

文档解析完后，存入数据库里

检索增强

![RAG](<Screenshot 2024-02-02 at 6.32.06 PM.png>)

### 文档问答算法流程

![文档问答](<Screenshot 2024-02-02 at 6.33.29 PM.png>)

切分方式更加细致：
拿出来文档名称、标题、正文
得到chunk_id, title_chain, paragraph

### 文档解析流程

![文档解析](<Screenshot 2024-02-02 at 6.40.45 PM.png>)

### 基于RAG的难点

1. 多模态数据的展示？

图片、文本、表格三种数据类型，传统方案是对三种数据类型分别建模，由于各个类型语义空间不匹配，效果就达不到预期

-> 将自然语言作为桥梁，将图像和表格映射到语言空间中
-> 利用业界各个模态的预训练模型，微调出Data2Text方法，将多模态数据转化为文本数据

![多模态数据的处理](<Screenshot 2024-02-02 at 6.46.07 PM.png>)

2. 索引的制作

文档通常具备标题、层级等信息

-> Doc2Bot。切分层级，然后对话标注，形成6000对话的5个领域

![Doc2Bot](<Screenshot 2024-02-02 at 6.49.04 PM.png>)

-> DocGraph： 通过文档的标题、层级、段落等信息，构建文档的图结构

3. 生成效果如何保障

现在的难点：
- 指令遵循能力：文档问答对输出的准确性、风格、格式有着复杂的要求。
- 生成的幻觉：回复里容易为答案额外增加一些文档里没有的信息。
- 生成结果的自动评测：传统的自动化评测无法针对LLM的输出进行准确打分。

指令遵循：SFT

幻觉：一般用RLHF，依赖RewardModel进行指导生成。但是在RAG文档问答领域，并不能反应文档问答的细微的错误
依靠人工标注数据，提出了新的Post-SFT方法

![处理幻觉](<Screenshot 2024-02-02 at 7.01.30 PM.png>)

生成结果的自动评测：传统的自动化评测无法针对LLM的输出进行准确打分。通过与标注同事交流，他们评估答案时，会从多个角度进行评分，然后再进行综合判断。所以我们让模型先生成要评价的角度，然后利用多Agent协作的方式，来进行综合判断

![评测角度](<Screenshot 2024-02-02 at 7.03.24 PM.png>)

答辩委员会评估方法

![答辩委员会评估方法](<Screenshot 2024-02-02 at 7.04.43 PM.png>)

# 答疑

RAG随着数据增加而性能下降的解决方案：
1. 核心是要解决搜索，训练自己的embedding模型，用开源的更好更大的embedding模型
2. 改写问题：华仔有多高 -> 刘德华的身高

Chunk的粒度：
有的企业问的细一点，就细一点。有的企业喜欢问摘要的，就切几千字都有可能

持续训练问题：
至少需要10B token以上的新数据再进行训练

幻觉的判别：
通过人来判别。幻觉的生成是因为模型训练时的把所有的数据都喂进去了。比如问“中国的首都是哪里”，历史上的首都是南京，但是现在是北京。模型可能会生成南京。模型分辨不出来哪个是对的。所以需要人来判别，需要人的知识、时间段来理解

基于成本和效果，目前使用RAG的方法

大模型的训练集如何评价好与不好：
1. 准不准
2. 多样性
3. 数据结构难不难

看到70min 在答疑
