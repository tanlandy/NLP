# 13多模态

## 基本概念

跨模态模型：TTS, ASR模型等等，在视觉、文本、音频三种模态之间相互转化的模型
单模态⼤模型：文本输出文本输出，视频输出视频输出
多模态模型：输出信号可以是图片/音频/文字，然后把多种模态映射到同一个线性空间里（重点是多模态的相同模态的对齐和不同模态之间差异的保持）
多模态语⾔⼤模型：由LLM扩展⽽来的具有接收与推理多模态信息能⼒的模型

前三者相互独立

## 里程碑

### 图像表示token化

![ViT](<截屏2024-03-15 下午4.40.11.png>)

将图像特征融入到了Transformer架构中

### 基于Transformer架构的图像-文本联合建模

![VisualBert](<截屏2024-03-15 下午4.44.44.png>)

### 图文token对齐模型

![Clip](<截屏2024-03-15 下午4.46.14.png>)

4亿token对

### 开域下的图像分类-目标检测-图像分割

难度越来越高

Clip的提出，出现了问生图任务的复兴

### 多模态大模型的出现

GPT-4 with Vision (GPT4v)
⽀持图-⽂交替输出

输⼊：可以接收⽂本、图像信息输⼊

输出：⾃然语⾔⽂本

不⽀持视频，但⽀持含多张图像的序列输⼊

特性：

1. 遵循文字指示
2. 理解视觉指向和参考
3. ⽀持视觉+⽂本联合提示
4. 少样本上下⽂学习
5. 强⼤的视觉认知能⼒
6. 时序视觉信号理解

![遵循文字指示](<截屏2024-03-15 下午5.33.14.png>)
![理解视觉指向和参考](<截屏2024-03-15 下午5.33.27.png>)

## GPT4v vs. Gemini

GPT4v回答语气更加柔和（更啰嗦），会有一些补充信息和思考过程的信息。一般不会生成偏向负面的情绪的内容。更讲究逻辑性和合理性

Gemini回答更笃定，不会专门生成思考过程和背景信息，所以在一些学术的benchmark上得分会更高。速度比GPT4v快很多

## 具身智能

把机器人看到的场景，以比如图片的形式输入给大模型，让大模型生成一些指令去操作这个机器人。也就是让大模型充当机器人的指令中枢，实现机器人的调度

- 具身智能场景：
  假设你是一个机器人，在厨房从事工作，你会执行的操作包括  靠近(物体坐标)， 抓取(物体坐标),  移动(开始坐标，结束坐标)，这里的坐标需要根据你的视觉系统来估计xy位置，以及深度信息z。人类会给你指令，你需要按照人类指令要求的完成对应的操作。比如，人类：把抽屉里的绿色包装袋拿出来。此时你看到的画面是这样的：
  请问接下来你该执行什么指令？只给出指令即可，但是需要包括具体的坐标信息（假设当前画面的长宽为单位1，使用你估计的深度信息以及xy偏移位置）

具身智能的场景，Gemini比GPT4v强很多，谷歌布局更早

看到100min，在答疑
