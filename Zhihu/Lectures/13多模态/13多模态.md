# 13多模态

## 基本概念

跨模态模型：TTS, ASR模型等等，在视觉、文本、音频三种模态之间相互转化的模型
单模态⼤模型：文本输出文本输出，视频输出视频输出
多模态模型：输出信号可以是图片/音频/文字，然后把多种模态映射到同一个线性空间里（重点是多模态的相同模态的对齐和不同模态之间差异的保持）
多模态语⾔⼤模型：由LLM扩展⽽来的具有接收与推理多模态信息能⼒的模型

前三者相互独立

## 里程碑

### 图像表示token化

![ViT](<截屏2024-03-15 下午4.40.11.png>)

将图像特征融入到了Transformer架构中

### 基于Transformer架构的图像-文本联合建模

![VisualBert](<截屏2024-03-15 下午4.44.44.png>)

### 图文token对齐模型

![Clip](<截屏2024-03-15 下午4.46.14.png>)

4亿token对

### 开域下的图像分类-目标检测-图像分割

难度越来越高

Clip的提出，出现了问生图任务的复兴

### 多模态大模型的出现

GPT-4 with Vision (GPT4v)
⽀持图-⽂交替输出

输⼊：可以接收⽂本、图像信息输⼊

输出：⾃然语⾔⽂本

不⽀持视频，但⽀持含多张图像的序列输⼊

特性：

1. 遵循文字指示
2. 理解视觉指向和参考
3. ⽀持视觉+⽂本联合提示
4. 少样本上下⽂学习
5. 强⼤的视觉认知能⼒
6. 时序视觉信号理解

![遵循文字指示](<截屏2024-03-15 下午5.33.14.png>)
![理解视觉指向和参考](<截屏2024-03-15 下午5.33.27.png>)

## GPT4v vs. Gemini

GPT4v回答语气更加柔和（更啰嗦），会有一些补充信息和思考过程的信息。一般不会生成偏向负面的情绪的内容。更讲究逻辑性和合理性

Gemini回答更笃定，不会专门生成思考过程和背景信息，所以在一些学术的benchmark上得分会更高。速度比GPT4v快很多

## 具身智能

把机器人看到的场景，以比如图片的形式输入给大模型，让大模型生成一些指令去操作这个机器人。也就是让大模型充当机器人的指令中枢，实现机器人的调度

- 具身智能场景：
  假设你是一个机器人，在厨房从事工作，你会执行的操作包括  靠近(物体坐标)， 抓取(物体坐标),  移动(开始坐标，结束坐标)，这里的坐标需要根据你的视觉系统来估计xy位置，以及深度信息z。人类会给你指令，你需要按照人类指令要求的完成对应的操作。比如，人类：把抽屉里的绿色包装袋拿出来。此时你看到的画面是这样的：
  请问接下来你该执行什么指令？只给出指令即可，但是需要包括具体的坐标信息（假设当前画面的长宽为单位1，使用你估计的深度信息以及xy偏移位置）

具身智能的场景，Gemini比GPT4v强很多，谷歌布局更早

## 表格/图片提取

[Table Transformer](https://huggingface.co/microsoft/table-transformer-detection)

用一个前置的table transformer把PDF抽取成结构化的数据，然后输入给LLM就可以

## LLaVa

目前多模态大语言模型的SoTA的基准。其他的多模态模型一般都是在LLaVa的基础上，使用更多的数据或者一些训练的trick进行开发实现的

LLaVa1.6：把一个切分成好几块，每一块都过一个Clip，从而实现更高分辨率图像

## 应用场景

具身智能

文本理解

复杂逻辑识别

# 图文多模态大语言模型的评测

MME评测集：

- 视觉认知Perception能力的测试
- 视觉问答任务（推理任务）

[榜单](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation)

评估多模态大语言模型的能力：

- 指令的遵从能力
- 视觉认知能力

# LLaVA图文对话系统搭建

多模态大语言模型的训练过程：

1. 特征对齐的预训练。只更新特征映射矩阵

- 8X A100耗时5.5h，基于DeepSpeed ZeRO2

2. 端到端微调。特征投影矩阵和LLM都进行更新

- 8X A100耗时20h，基于DeepSpeed ZeRO3

![数据处理1](<截屏2024-03-20 下午3.30.10.png>)

![数据处理2](<截屏2024-03-20 下午3.31.56.png>)

Captions：基于图片的短文本生成。这个方面传统的方式已经效果非常好了。推荐模型：RAM, RAM++
目标检测：图片中有哪些物体，每个物体的几何坐标。这个问题目前的方式效果也非常好了。推荐模型：GroungDINO

LLaVA不适合微调训练，特别容易过拟合。本来参数量就比较小，适合从头训练

Serve: LLaVA-v1.6

改进：

1. Vision Encoder改进

LLaVA-v1.6
Fuyu-8B

2. Projection特征映射矩阵的改进

InternLM-XComposer2

看到多模态（中）的68min
