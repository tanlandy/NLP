# 模型数据集下载

[source参考来源](https://zhuanlan.zhihu.com/p/663712983)

## hfd

[Huggingface镜像站](https://hf-mirror.com/)

参考页面的方法三，使用hfd下载模型和数据集

### 下载hfd

```shell
wget https://hf-mirror.com/hfd/hfd.sh
chmod a+x hfd.sh
```

### 设置环境变量

Windows Powershell

```shell
$env:HF_ENDPOINT = "https://hf-mirror.com"
```

### 切换到要存储的本地文件夹

```shell
cd /data/models
```

### 下载模型

Linux

```shell
export HF_ENDPOINT=https://hf-mirror.com
```

```shell
./hfd.sh mistralai/Mistral-7B-v0.1 --hf_username linlinlin --hf_token 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 --tool aria2c -x 4 
```

### 下载数据集

Linux

```shell
export HF_HUB_ENABLE_HF_TRANSFER=1
```

```shell
huggingface-cli download --resume-download --repo-type dataset --local-dir-use-symlinks False tatsu-lab/alpaca_eval --local-dir /Users/lin/Documents/models/model_directory/alpaca_eval
```

# 使用已下载到本地的模型/数据集

## 模型

```python
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
from transformers import AutoTokenizer, AutoModel
from transformers import AutoModelForCausalLM

MODEL_NAME = "/opt/models/gpt2"  # 对应下载时存储的本地文件夹

# MODEL_NAME = "gpt2"  # 同样可以用这个方式，模型在.cache里有符号链接

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, local_files_only=True)

# Model
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, local_files_only=True)

```

## 数据集

```python
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import datasets
from datasets import load_dataset

DATASET_NAME = "/opt/models/datasets/rotten_tomatoes"  # 对应下载时存储的本地文件夹

# DATASET_NAME = "rotten_tomatoes"  # 同样可以用这个方式，数据集在.cache里有符号链接

raw_datasets = load_dataset(DATASET_NAME, local_files_only=True)

```

# need a GPU for local models

alpaca_eval evaluate_from_model \
  --model_configs '/opt/Projects/SFT/data/zephyr-7b-sft-lora-Herme-v0.2' \
  --annotators_config 'alpaca_eval_gpt4_turbo_fn'
