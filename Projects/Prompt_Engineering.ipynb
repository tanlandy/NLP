{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QKzu6MyXwyS7"
      },
      "source": [
        "# 生成式AI应用：总结对话 Summarize Dialogue\n",
        "\n",
        "使用FLAN-T5(参数量251M)模型，利用[DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum)数据库（包含12.5k行的训练集，1.5k测试集, 0.5k验证集），用Prompt工程，通过调整input text来优化模型在dialogue summary这一子任务上的效果。\n",
        "\n",
        "本项目中的Prompt工程包括instruction prompt中zero shot, one shot, few shot推断。\n",
        "\n",
        "最末尾对比了no prompt model, zero shot, one shot, two shot, five shot的结果。\n",
        "\n",
        "项目资源：colab notebook, T4 GPU, 15GB GPU RAM。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y3iyzp-_yKj-"
      },
      "source": [
        "# 1 - 库和数据准备\n",
        "\n",
        "导库，准备数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRXfyok8yUUw"
      },
      "outputs": [],
      "source": [
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    datasets==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DzVXh6siyaNR"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzqvKO900UlB"
      },
      "outputs": [],
      "source": [
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "\n",
        "dataset = load_dataset(huggingface_dataset_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tnlpMDZRHe_e"
      },
      "source": [
        "看一下数据格式和大小，一共有4列，除了dialogue和summary外，还有一个topic的特征"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVcStyfM0ZZw",
        "outputId": "aba148da-e833-46ac-cf09-54af88c86eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
            "        num_rows: 12460\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
            "        num_rows: 1500\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXezcNjH06uo",
        "outputId": "8c4c020c-8647-4b25-b13b-06932d75fc72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------\n",
            "Example 1\n",
            "-------------------\n",
            "DIALOGUE:\n",
            "\n",
            "#Person1#: Hey, Tom, what to go for a run?\n",
            "#Person2#: No thanks. I like to run in the morning. I ran a couple of miles when I woke up today.\n",
            "#Person1#: I try to do that, but I can't get up early enough.\n",
            "#Person2#: I couldn't either at first, but you get used to it.\n",
            "#Person1#: It's so hot at lunchtime ; I'd rather run in the morning.\n",
            "#Person2#: Well, why don't you come tomorrow? I'll stop by your house on my way out.\n",
            "#Person1#: I could try, but I can't say for sure if I'll get up in time. What time do you want to go?\n",
            "#Person2#: I'll give you a call around 6 o'clock and stop by around 6 thirty.\n",
            "#Person1#: O. K. , maybe if I have someone to go with, I'll be able to get up in time for a jog.\n",
            "#Person2#: Great, I'll see you then.\n",
            "#Person1#: See you.\n",
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "\n",
            "Tom invites #Person1# to run in the morning. #Person1# would try to get up and join him.\n",
            "-------------------\n",
            "\n",
            "-------------------\n",
            "Example 2\n",
            "-------------------\n",
            "DIALOGUE:\n",
            "\n",
            "#Person1#: I'm frustrated. We're supposed to do our assignment on the computer, but I have difficulty getting access to the computers in the library.\n",
            "#Person2#: I understand the way you feel. I'm looking forward to the day when I can afford to get my own.\n",
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "\n",
            "#Person1# has difficulty getting access to the computers in the library to do #Person1#'s assignment.\n",
            "-------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example_indices = [666, 999]\n",
        "\n",
        "dash_line = '-'.join('' for x in range(20))\n",
        "\n",
        "for i, index in enumerate(example_indices):\n",
        "    print(dash_line)\n",
        "    print('Example', i + 1)\n",
        "    print(dash_line)\n",
        "    print('DIALOGUE:\\n')\n",
        "    print(dataset['test'][index]['dialogue'])\n",
        "    print(dash_line)\n",
        "    print('HUMAN SUMMARY:\\n')\n",
        "    print(dataset['test'][index]['summary'])\n",
        "    print(dash_line)\n",
        "    print()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rwOSB1iDHs90"
      },
      "source": [
        "上个cell把dialogue和summary列打印了几个样本，可以从中知道样本的大致样子。\n",
        "\n",
        "接下来从HuggingFace库里加载Flan-T5模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0xyUu0K1nHp"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "\n",
        "model_name='google/flan-t5-base'\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpDSFukD2AMt"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DI-Vwp-JIGQY"
      },
      "source": [
        "`tokenizer`的功能效果：将单词向量化。需要注意的是不同模型的`tokenizer`有所不同，要使用对应的`tokenizer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ICGlvoI2UBf",
        "outputId": "187ad58f-930a-489c-918f-8a2285205ab0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ENCODED SENTENCE:\n",
            "tensor([ 363,   97,   19,   34,    6, 3059,   58,    1], device='cuda:0')\n",
            "\n",
            "DECODED SENTENCE:\n",
            "What time is it, Tom?\n"
          ]
        }
      ],
      "source": [
        "sentence = \"What time is it, Tom?\"\n",
        "\n",
        "\n",
        "# 将上述句子向量化\n",
        "sentence_encoded = tokenizer(sentence, return_tensors='pt').to(device)\n",
        "\n",
        "# 解码\n",
        "sentence_decoded = tokenizer.decode(\n",
        "        sentence_encoded[\"input_ids\"][0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "print('ENCODED SENTENCE:')\n",
        "print(sentence_encoded[\"input_ids\"][0])\n",
        "print('\\nDECODED SENTENCE:')\n",
        "print(sentence_decoded)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CQOjMoTOznEk"
      },
      "source": [
        "# 2 - Baseline\n",
        "\n",
        "不使用任何Prompt工程"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J-DNZ1v-2by8"
      },
      "source": [
        "Prompt engineering是针对给定任务，人为调整prompt(input)来改善模型结果的过程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjvZHeOr3Jri",
        "outputId": "bad94eee-67aa-40b2-a3d8-ceb76fe7ee9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------\n",
            "Example  1\n",
            "-------------------\n",
            "DIALOGUE:\n",
            "#Person1#: Hey, Tom, what to go for a run?\n",
            "#Person2#: No thanks. I like to run in the morning. I ran a couple of miles when I woke up today.\n",
            "#Person1#: I try to do that, but I can't get up early enough.\n",
            "#Person2#: I couldn't either at first, but you get used to it.\n",
            "#Person1#: It's so hot at lunchtime ; I'd rather run in the morning.\n",
            "#Person2#: Well, why don't you come tomorrow? I'll stop by your house on my way out.\n",
            "#Person1#: I could try, but I can't say for sure if I'll get up in time. What time do you want to go?\n",
            "#Person2#: I'll give you a call around 6 o'clock and stop by around 6 thirty.\n",
            "#Person1#: O. K. , maybe if I have someone to go with, I'll be able to get up in time for a jog.\n",
            "#Person2#: Great, I'll see you then.\n",
            "#Person1#: See you.\n",
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "Tom invites #Person1# to run in the morning. #Person1# would try to get up and join him.\n",
            "-------------------\n",
            "BASELINE MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
            "Person1#: Hey, Tom. What to do for a run?\n",
            "\n",
            "-------------------\n",
            "Example  2\n",
            "-------------------\n",
            "DIALOGUE:\n",
            "#Person1#: I'm frustrated. We're supposed to do our assignment on the computer, but I have difficulty getting access to the computers in the library.\n",
            "#Person2#: I understand the way you feel. I'm looking forward to the day when I can afford to get my own.\n",
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "#Person1# has difficulty getting access to the computers in the library to do #Person1#'s assignment.\n",
            "-------------------\n",
            "BASELINE MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
            "Person1: I'm frustrated. We're supposed to do our assignment on the computer, but I have difficulty getting access to the computers in the library.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    # 使用Flan-T5的tokenizer，把dialogue向量化\n",
        "    inputs = tokenizer(dialogue, return_tensors='pt').to(device)\n",
        "    # 把inputs喂给Flan-T5，得到结果\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'DIALOGUE:\\n{dialogue}')\n",
        "    print(dash_line)\n",
        "    print(f'HUMAN SUMMARY:\\n{summary}')\n",
        "    print(dash_line)\n",
        "    print(f'BASELINE MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tb4e_Jo4CCM"
      },
      "source": [
        "# 3 - Instruction Prompt\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fzRWmXXg6sVH"
      },
      "source": [
        "\n",
        "## 3.1 - Zero Shot Inference\n",
        "\n",
        "将input结构化，在input中例子的数量为0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL_cLfCd5mhQ",
        "outputId": "1b41bebc-cbfb-46ef-e19a-882ff7946266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------\n",
            "Example  1\n",
            "-------------------\n",
            "INPUT PROMPT:\n",
            "\n",
            "        Summarize the following conversation.\n",
            "\n",
            "        #Person1#: Hey, Tom, what to go for a run?\n",
            "#Person2#: No thanks. I like to run in the morning. I ran a couple of miles when I woke up today.\n",
            "#Person1#: I try to do that, but I can't get up early enough.\n",
            "#Person2#: I couldn't either at first, but you get used to it.\n",
            "#Person1#: It's so hot at lunchtime ; I'd rather run in the morning.\n",
            "#Person2#: Well, why don't you come tomorrow? I'll stop by your house on my way out.\n",
            "#Person1#: I could try, but I can't say for sure if I'll get up in time. What time do you want to go?\n",
            "#Person2#: I'll give you a call around 6 o'clock and stop by around 6 thirty.\n",
            "#Person1#: O. K. , maybe if I have someone to go with, I'll be able to get up in time for a jog.\n",
            "#Person2#: Great, I'll see you then.\n",
            "#Person1#: See you.\n",
            "\n",
            "        Summary:\n",
            "        \n",
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "Tom invites #Person1# to run in the morning. #Person1# would try to get up and join him.\n",
            "-------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "Person1#: Hi, Tom. I'm looking for a jog.\n",
            "\n",
            "-------------------\n",
            "Example  2\n",
            "-------------------\n",
            "INPUT PROMPT:\n",
            "\n",
            "        Summarize the following conversation.\n",
            "\n",
            "        #Person1#: I'm frustrated. We're supposed to do our assignment on the computer, but I have difficulty getting access to the computers in the library.\n",
            "#Person2#: I understand the way you feel. I'm looking forward to the day when I can afford to get my own.\n",
            "\n",
            "        Summary:\n",
            "        \n",
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "#Person1# has difficulty getting access to the computers in the library to do #Person1#'s assignment.\n",
            "-------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "#Person1: I'm frustrated. I'm looking forward to the day when I can afford to get my own computer.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    # input输入不再仅仅是dialogue, 而是在其首尾加入了一个\"引导\"的prompt\n",
        "    prompt = f\"\"\"\n",
        "        Summarize the following conversation.\n",
        "\n",
        "        {dialogue}\n",
        "\n",
        "        Summary:\n",
        "        \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'INPUT PROMPT:\\n{prompt}')\n",
        "    print(dash_line)\n",
        "    print(f'HUMAN SUMMARY:\\n{summary}')\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c9uJbi406FKw"
      },
      "source": [
        "## 3.2 - Zero Shot Inference with another template"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qFivJNWw65yO"
      },
      "source": [
        "可以在[pre-built FLAN-T5 prompts](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py)找到其他的模板，这里再尝试一个新的instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAR1TAH77G46",
        "outputId": "cf4180b7-94bf-4b0f-c9a5-9252a7c9f1bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------\n",
            "Example  1\n",
            "-------------------\n",
            "INPUT PROMPT:\n",
            "\n",
            "        Dialogue:\n",
            "\n",
            "        #Person1#: Hey, Tom, what to go for a run?\n",
            "#Person2#: No thanks. I like to run in the morning. I ran a couple of miles when I woke up today.\n",
            "#Person1#: I try to do that, but I can't get up early enough.\n",
            "#Person2#: I couldn't either at first, but you get used to it.\n",
            "#Person1#: It's so hot at lunchtime ; I'd rather run in the morning.\n",
            "#Person2#: Well, why don't you come tomorrow? I'll stop by your house on my way out.\n",
            "#Person1#: I could try, but I can't say for sure if I'll get up in time. What time do you want to go?\n",
            "#Person2#: I'll give you a call around 6 o'clock and stop by around 6 thirty.\n",
            "#Person1#: O. K. , maybe if I have someone to go with, I'll be able to get up in time for a jog.\n",
            "#Person2#: Great, I'll see you then.\n",
            "#Person1#: See you.\n",
            "\n",
            "        What was going on?\n",
            "        \n",
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "Tom invites #Person1# to run in the morning. #Person1# would try to get up and join him.\n",
            "\n",
            "-------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "Person1 wants to go for a run in the morning.\n",
            "\n",
            "-------------------\n",
            "Example  2\n",
            "-------------------\n",
            "INPUT PROMPT:\n",
            "\n",
            "        Dialogue:\n",
            "\n",
            "        #Person1#: I'm frustrated. We're supposed to do our assignment on the computer, but I have difficulty getting access to the computers in the library.\n",
            "#Person2#: I understand the way you feel. I'm looking forward to the day when I can afford to get my own.\n",
            "\n",
            "        What was going on?\n",
            "        \n",
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "#Person1# has difficulty getting access to the computers in the library to do #Person1#'s assignment.\n",
            "\n",
            "-------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "Person1 is frustrated because they can't access the computers in the library.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    # 最后一句话和3.1有所不同\n",
        "    prompt = f\"\"\"\n",
        "        Dialogue:\n",
        "\n",
        "        {dialogue}\n",
        "\n",
        "        What was going on?\n",
        "        \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'INPUT PROMPT:\\n{prompt}')\n",
        "    print(dash_line)\n",
        "    print(f'HUMAN SUMMARY:\\n{summary}\\n')\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6DIjqCjO7I8z"
      },
      "source": [
        "结果与使用模板一稍有区别，使用第三人称的口吻，结果更合理，接下来尝试one shot和few shot"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro5Bgzz67WvW"
      },
      "source": [
        "# 4 - One Shot and Few Shot"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lUo5KAsE78fA"
      },
      "source": [
        "在input中给到LLM例子。如果给到了一个例子，那就是one shot，多个例子是few shot。注意例子并不是越多越好，并且例子数量会受限于input window size。\n",
        "\n",
        "这个方法叫\"in-context learning\"，可参考[blog from HuggingFace](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api).\n",
        "\n",
        "首先编写一个把例子添加到Prompt的函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iqdJn6WF8jTY"
      },
      "outputs": [],
      "source": [
        "def make_prompt(example_indices_full, example_index_to_summarize):\n",
        "    prompt = ''\n",
        "    for index in example_indices_full:\n",
        "        dialogue = dataset['test'][index]['dialogue']\n",
        "        summary = dataset['test'][index]['summary']\n",
        "\n",
        "        # 注意FLAN-T5模型的截止需要用'{summary}\\n\\n\\n'\n",
        "        prompt += f\"\"\"\n",
        "            Dialogue:\n",
        "\n",
        "            {dialogue}\n",
        "\n",
        "            What was going on?\n",
        "            {summary}\n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
        "\n",
        "    prompt += f\"\"\"\n",
        "        Dialogue:\n",
        "\n",
        "        {dialogue}\n",
        "\n",
        "        What was going on?\n",
        "        \"\"\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lF1XB6gC8fZ_"
      },
      "source": [
        "## 4.1 - One Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DorWLuSZ_svN",
        "outputId": "58b2c888-2f5f-4629-af33-f98e882427c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "            Dialogue:\n",
            "\n",
            "            #Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "            What was going on?\n",
            "            #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "\n",
            "\n",
            "            \n",
            "        Dialogue:\n",
            "\n",
            "        #Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "        What was going on?\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "example_indices_full = [40]  # 加入一个例子\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
        "\n",
        "print(one_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVo06cbdALdx",
        "outputId": "c2607f5e-693e-41c7-f8ce-c01987561120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "\n",
            "-------------------\n",
            "MODEL GENERATION - ONE SHOT:\n",
            "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n"
          ]
        }
      ],
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(one_shot_prompt, return_tensors='pt').to(device)\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line)\n",
        "print(f'HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OhzWpV_3Ajg5"
      },
      "source": [
        "## 4.2 - Few Shots"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BxbCJz1_A2i8"
      },
      "source": [
        "### 4.2.1 - Two Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I6zaavYBNFK",
        "outputId": "9fb8c64e-2633-4b86-81df-17447e797e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "            Dialogue:\n",
            "\n",
            "            #Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "            What was going on?\n",
            "            #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "\n",
            "\n",
            "            \n",
            "            Dialogue:\n",
            "\n",
            "            #Person1#: May, do you mind helping me prepare for the picnic?\n",
            "#Person2#: Sure. Have you checked the weather report?\n",
            "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
            "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
            "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
            "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
            "#Person1#: All set. May, can you help me take all these things to the living room?\n",
            "#Person2#: Yes, madam.\n",
            "#Person1#: Ask Daniel to give you a hand?\n",
            "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
            "\n",
            "            What was going on?\n",
            "            Mom asks May to help to prepare for the picnic and May agrees.\n",
            "\n",
            "\n",
            "            \n",
            "        Dialogue:\n",
            "\n",
            "        #Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "        What was going on?\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "example_indices_full = [40, 80]  # 加入两个例子\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "two_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
        "\n",
        "print(two_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOxnfRV5CYe4",
        "outputId": "4884aa66-4d19-47c2-91a0-595da97c7eaa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "\n",
            "-------------------\n",
            "MODEL GENERATION - TWO SHOT:\n",
            "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n"
          ]
        }
      ],
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(two_shot_prompt, return_tensors='pt').to(device)\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line)\n",
        "print(f'HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - TWO SHOT:\\n{output}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nT2Fi_hjA6HO"
      },
      "source": [
        "### 4.2.2 - Three Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "k72a94KNBS_x"
      },
      "outputs": [],
      "source": [
        "example_indices_full = [40, 80, 120]\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "three_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
        "\n",
        "# print(three_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_ZTBYm1CnKl",
        "outputId": "abf50854-3dd2-4e98-a806-70ee815325a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "\n",
            "-------------------\n",
            "MODEL GENERATION - THREE SHOT:\n",
            "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n"
          ]
        }
      ],
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(three_shot_prompt, return_tensors='pt').to(device)\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line)\n",
        "print(f'HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - THREE SHOT:\\n{output}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eAtmmKADA8lF"
      },
      "source": [
        "### 4.2.3 - Five Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zdkFrl_dBWNo"
      },
      "outputs": [],
      "source": [
        "example_indices_full = [x for x in range(40, 200, 32)]\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "five_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
        "\n",
        "# print(five_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FXUsan9CuM9",
        "outputId": "a6243cdd-9520-4914-cd5c-cd7bc93e80b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "\n",
            "-------------------\n",
            "MODEL GENERATION - FIVE SHOT:\n",
            "#Person1 recommends upgrading their system and hardware.\n"
          ]
        }
      ],
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(five_shot_prompt, return_tensors='pt').to(device)\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line)\n",
        "print(f'HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - FIVE SHOT:\\n{output}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jD9JKhwaA_J9"
      },
      "source": [
        "### 4.2.4 - Ten Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "266rHGsgBctW"
      },
      "outputs": [],
      "source": [
        "example_indices_full = [x for x in range(40, 200, 16)]\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "ten_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
        "\n",
        "# print(five_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WSN-KlbC1Wn",
        "outputId": "bf8b9e93-d885-4982-b2fd-44a49b17f671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "\n",
            "-------------------\n",
            "MODEL GENERATION - TEN SHOT:\n",
            "#Person1 recommends upgrading the system and hardware.\n"
          ]
        }
      ],
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(ten_shot_prompt, return_tensors='pt').to(device)\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line)\n",
        "print(f'HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - TEN SHOT:\\n{output}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ibhAaHsvC4z-"
      },
      "source": [
        "# 5 - Comparison"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uKd3lGjUD1gi"
      },
      "source": [
        "在一个结果中，对比baseline model, zero shot, one shot, two shot, five shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oWjLbJ6Dc78",
        "outputId": "bbbeee4b-e4df-45c4-cf2c-010eb9319045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "Tom invites #Person1# to run in the morning. #Person1# would try to get up and join him.\n",
            "-------------------\n",
            "BASELINE MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
            "Person1#: Hey, Tom. What to do for a run?\n",
            "-------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "Person1#: Hi, Tom. I'm looking for a jog.\n",
            "-------------------\n",
            "MODEL GENERATION - ONE SHOT:\n",
            "Person1 wants to go for a run in the morning.\n",
            "-------------------\n",
            "MODEL GENERATION - TWO SHOT:\n",
            "Person1 wants to go for a run in the morning.\n",
            "-------------------\n",
            "MODEL GENERATION - FIVE SHOT:\n",
            "Person1 wants to go for a run in the morning.\n",
            "\n",
            "\n",
            "-------------------\n",
            "HUMAN SUMMARY:\n",
            "#Person1# has difficulty getting access to the computers in the library to do #Person1#'s assignment.\n",
            "-------------------\n",
            "BASELINE MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
            "Person1: I'm frustrated. We're supposed to do our assignment on the computer, but I have difficulty getting access to the computers in the library.\n",
            "-------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "#Person1: I'm frustrated. I'm looking forward to the day when I can afford to get my own computer.\n",
            "-------------------\n",
            "MODEL GENERATION - ONE SHOT:\n",
            "Person1 is frustrated because they are supposed to do their assignment on the computer.\n",
            "-------------------\n",
            "MODEL GENERATION - TWO SHOT:\n",
            "Person1 is frustrated because they are supposed to do their assignment on the computer, but they have difficulty getting access to the computers in the library.\n",
            "-------------------\n",
            "MODEL GENERATION - FIVE SHOT:\n",
            "#Person1 is frustrated because they are supposed to do their assignment on the computer, but they have difficulty getting access to the computers in the library.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "\n",
        "    inputs = tokenizer(dialogue, return_tensors='pt').to(device)\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    print(dash_line)\n",
        "    print(f'HUMAN SUMMARY:\\n{summary}')\n",
        "    print(dash_line)\n",
        "    print(f'BASELINE MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}')\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "        Summarize the following conversation.\n",
        "\n",
        "        {dialogue}\n",
        "\n",
        "        Summary:\n",
        "        \"\"\"\n",
        "\n",
        "    # Zero Shot\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n",
        "\n",
        "    # One Shot\n",
        "    one_shot_prompt = make_prompt([40], index)\n",
        "    inputs_one_shot = tokenizer(one_shot_prompt, return_tensors='pt').to(device)\n",
        "    output_one_shot = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs_one_shot[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - ONE SHOT:\\n{output_one_shot}')\n",
        "\n",
        "    # Two Shot\n",
        "    two_shot_prompt = make_prompt([40, 80], index)\n",
        "    inputs_two_shot = tokenizer(two_shot_prompt, return_tensors='pt').to(device)\n",
        "    output_two_shot = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs_two_shot[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - TWO SHOT:\\n{output_two_shot}')\n",
        "\n",
        "    # Five Shot\n",
        "    five_shot_prompt = make_prompt([x for x in range(40, 200, 32)], index)\n",
        "    inputs_five_shot = tokenizer(five_shot_prompt, return_tensors='pt').to(device)\n",
        "    output_five_shot = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs_five_shot[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - FIVE SHOT:\\n{output_five_shot}\\n\\n')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tfGqyMurL-3-"
      },
      "source": [
        "定性来看，few shot结果好于zero shot好于baseline。\n",
        "\n",
        "更多shot不一定能带来更好的结果。\n",
        "\n",
        "之后可以使用ROUGE来定量计算结果的变化。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
